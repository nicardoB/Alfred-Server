# Alfred Server - Local Development Environment

## ğŸš€ Quick Start

### Start Development Environment
**Double-click `dev-start.command`** in Finder

This will:
- âœ… Start Ollama (local AI models)
- âœ… Start Alfred Server (port 3001)
- âœ… Start Chat Client (port 3000)
- âœ… Create/verify owner account
- âœ… Open browser automatically

### Stop Development Environment
**Double-click `dev-stop.command`** in Finder

This will:
- âœ… Stop Alfred Server
- âœ… Stop Chat Client
- âœ… Clean up processes
- âœ… Option to stop Ollama
- âœ… Clean up logs

## ğŸ”‘ Login Credentials

- **Email**: `owner@local.test`
- **Password**: `LocalDev#2024`

## ğŸ“Š Service URLs

- **Chat Client**: http://localhost:3000
- **Alfred Server API**: http://localhost:3001
- **Swagger UI**: http://localhost:3001/api-docs
- **Ollama**: http://localhost:11434

## ğŸ—ï¸ Architecture

```
Local Development Stack:
â”œâ”€â”€ Chat Client (localhost:3000) - Next.js frontend
â”œâ”€â”€ Alfred Server (localhost:3001) - Node.js API + SQLite
â”œâ”€â”€ Ollama (localhost:11434) - Local AI models
â””â”€â”€ All communication via localhost (no network complexity)
```

## ğŸ§ª Testing Workflow

1. **Start environment**: Double-click `dev-start.command`
2. **Browser opens** to chat client automatically
3. **Login** with credentials above
4. **Send test message**: "Hello, which AI provider are you using?"
5. **Expect**: Fast response from Ollama (local, free)

## ğŸ”§ Manual Commands (Alternative)

If you prefer terminal commands:

```bash
# Start everything
./dev-start.command

# Stop everything
./dev-stop.command

# View logs
tail -f /tmp/alfred-server.log
tail -f /tmp/alfred-chat.log
```

## ğŸ—‚ï¸ Project Structure

```
Alfred-Server/
â”œâ”€â”€ dev-start.command      # One-click startup
â”œâ”€â”€ dev-stop.command       # One-click shutdown
â”œâ”€â”€ src/                   # Server source code
â”œâ”€â”€ data/                  # SQLite database
â”œâ”€â”€ package.json           # Dependencies
â””â”€â”€ .env                   # Environment variables

../alfred-chat/
â”œâ”€â”€ src/                   # Chat client source
â”œâ”€â”€ .env.local            # Points to localhost:3001
â””â”€â”€ package.json          # Dependencies
```

## ğŸš¨ Troubleshooting

### Port Conflicts
```bash
# Check what's using ports
lsof -nP -iTCP:3000,3001,11434 -sTCP:LISTEN

# Kill conflicting processes
pkill -f "node.*server.js" || true
pkill -f "next dev" || true
pkill -f "ollama serve" || true
```

### Database Issues
```bash
# Reset database (loses data)
rm -rf ./data/
mkdir -p ./data/

# Restart with fresh database
NODE_ENV=development FORCE_DB_SYNC=true ./dev-start.command
```

### Missing Models
```bash
# Download required model
ollama pull llama3.1
```

## ğŸ¯ Daily Workflow

1. **Morning**: Double-click `dev-start.command`
2. **Development**: Code, test, iterate
3. **Evening**: Double-click `dev-stop.command`

## ğŸ”„ Next Steps (Future)

- **Production deployment** to Railway
- **Cloud Ollama** on Oracle Cloud
- **Multi-provider testing** (OpenAI, Claude)
- **Performance optimization**

---

This local setup provides a **fast, reliable, cost-free** development environment using your local machine's resources.
