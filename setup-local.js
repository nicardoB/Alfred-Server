#!/usr/bin/env node

import fs from 'fs';
import path from 'path';
import { fileURLToPath } from 'url';
import crypto from 'crypto';

const __filename = fileURLToPath(import.meta.url);
const __dirname = path.dirname(__filename);

console.log('🚀 Setting up Alfred Server for local development...\n');

// 1. Create data directory for SQLite database
const dataDir = path.join(__dirname, 'data');
if (!fs.existsSync(dataDir)) {
  fs.mkdirSync(dataDir, { recursive: true });
  console.log('✅ Created data directory for SQLite database');
} else {
  console.log('✅ Data directory already exists');
}

// 2. Create .env file for local development
const envPath = path.join(__dirname, '.env');
if (!fs.existsSync(envPath)) {
  const jwtSecret = crypto.randomBytes(64).toString('hex');
  const apiKeySalt = crypto.randomBytes(32).toString('hex');
  const monitoringApiKey = crypto.randomBytes(32).toString('hex');
  
  const envContent = `# Alfred Server - Local Development Configuration
# Generated by setup-local.js on ${new Date().toISOString()}

# Server Configuration
PORT=3001
NODE_ENV=development

# Database Configuration (SQLite for local development)
# DATABASE_URL is intentionally commented out to use SQLite
# DATABASE_URL=postgresql://username:password@localhost:5432/alfred_db

# Security (auto-generated)
JWT_SECRET=${jwtSecret}
API_KEY_SALT=${apiKeySalt}
MONITORING_API_KEY=${monitoringApiKey}

# AI Provider API Keys (add your own)
# ANTHROPIC_API_KEY=your_claude_api_key_here
# OPENAI_API_KEY=your_openai_api_key_here
# GITHUB_TOKEN=your_github_token_here

# Local Ollama Configuration
OLLAMA_BASE_URL=http://localhost:11434

# Smart AI Router Configuration
CLAUDE_MODEL=claude-3-sonnet-20240229
GPT_MODEL=gpt-4o-mini
COPILOT_MODEL=gpt-4

# Rate Limiting (generous for local development)
RATE_LIMIT_WINDOW_MS=60000
RATE_LIMIT_MAX_REQUESTS=1000

# Email Configuration (optional - for cost monitoring alerts)
# SENDGRID_API_KEY=your_sendgrid_api_key_here
# EMAIL_FROM=alfred@yourdomain.com
# EMAIL_TO=your-email@domain.com

# Audio Processing
MAX_AUDIO_CHUNK_SIZE=1024
SUPPORTED_AUDIO_FORMATS=PCM_16BIT,OPUS,AAC
DEFAULT_SAMPLE_RATE=16000
`;

  fs.writeFileSync(envPath, envContent);
  console.log('✅ Created .env file with local development configuration');
} else {
  console.log('✅ .env file already exists');
}

// 3. Check if Ollama is available
console.log('\n🔍 Checking system requirements...');

try {
  const { execSync } = await import('child_process');
  
  // Check if Ollama is installed
  try {
    execSync('which ollama', { stdio: 'ignore' });
    console.log('✅ Ollama is installed');
    
    // Check if Ollama service is running
    try {
      execSync('curl -s http://localhost:11434/api/tags', { stdio: 'ignore' });
      console.log('✅ Ollama service is running on localhost:11434');
    } catch (error) {
      console.log('⚠️  Ollama is installed but not running. Start it with: ollama serve');
    }
  } catch (error) {
    console.log('⚠️  Ollama not found. Install it with: brew install ollama');
  }
  
  // Check Node.js version
  const nodeVersion = process.version;
  const majorVersion = parseInt(nodeVersion.slice(1).split('.')[0]);
  if (majorVersion >= 18) {
    console.log(`✅ Node.js ${nodeVersion} (compatible)`);
  } else {
    console.log(`⚠️  Node.js ${nodeVersion} (requires >= 18.0.0)`);
  }
  
} catch (error) {
  console.log('⚠️  Could not check system requirements');
}

console.log('\n🎉 Local setup complete!');
console.log('\n📋 Next steps:');
console.log('1. Install Ollama (if not already installed): brew install ollama');
console.log('2. Start Ollama service: ollama serve');
console.log('3. Download a model: ollama pull llama3.1:7b');
console.log('4. Add your AI API keys to .env file (optional)');
console.log('5. Start Alfred Server: npm start');
console.log('\n🌐 Alfred Server will run on: http://localhost:3001');
console.log('📊 Dashboard will be available at: http://localhost:3001/dashboard');
console.log('\n💡 For production deployment, see TASKS.md for Railway setup instructions.');
